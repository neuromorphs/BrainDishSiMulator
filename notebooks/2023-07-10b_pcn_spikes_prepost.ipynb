{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import psweep as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/utils')  # Add the utils directory to the Python path\n",
    "import utils_data, utils_spikes, utils_events, utils_tensor, utils_pcn \n",
    "\n",
    "sys.path.append('../code/models')  # Add the models directory to the Python path\n",
    "import pcn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91b1c31",
   "metadata": {},
   "source": [
    "# Damn it's working predictive coding is really genius work \n",
    "## Now let's try to learn pre and post structures, but also to merge dishes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac8d02",
   "metadata": {},
   "source": [
    "## Merging dishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406af944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment specific parameters \n",
    "chip_ids = [9501, 11614, 11615] # experiment ID\n",
    "chip_sessions = [0,2] # 2 for post-training, 0 for pre-training\n",
    "\n",
    "# Stable parameters\n",
    "data_path = '../data/cortical_labs_data/' # path to data\n",
    "fs = 20000 # sampling frequency\n",
    "binsize = 100 # ms, bin size for spike counts\n",
    "array_size = 1024 # number of electrode in the array\n",
    "\n",
    "# Torch parameters \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "num_workers = 2\n",
    "pin_memory = False\n",
    "\n",
    "# Learn parameters\n",
    "batch_size = 32\n",
    "epochs = 5  # should be enough given the 07-07 notebook\n",
    "n_inferences_steps = 500  # number of inference steps per input\n",
    "n_generative_steps = 500 # number of generative steps per input\n",
    "\n",
    "# Layer parameters\n",
    "update_weights_flag = True  # whether to learn the FF weights\n",
    "f = utils_pcn.tanh\n",
    "df = utils_pcn.tanh_deriv\n",
    "\n",
    "# Network parameters\n",
    "fixed_predictions = True  # change the predictions or not\n",
    "theta_lr = 5e-4\n",
    "mu_lr = 5e-2\n",
    "weight_clamp = 50  # weight clamp\n",
    "mu_clamp = 1000  # value neuron clamp\n",
    "\n",
    "L1_size = 256\n",
    "L2_size = 128\n",
    "L3_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cab49835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading for chip 9501, session 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 26.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 5431.24it/s]\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 6238.27it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 6666.09it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 5696.92it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 6532.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      "Loading for chip 9501, session 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 26.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 6071.98it/s]\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 6248.68it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 6248.59it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 6056.84it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 6035.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      "Loading for chip 11614, session 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 25.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 6503.74it/s]\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 6248.59it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 6283.98it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 6248.68it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 6311.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      "Loading for chip 11614, session 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 25.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 6287.44it/s]\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 5940.35it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 5968.50it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 5963.41it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 5797.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      "Loading for chip 11615, session 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 25.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Could not load chip 11615, session 0<<\n",
      "------------------------\n",
      "\n",
      "Loading for chip 11615, session 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 22.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 7351.13it/s]\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 8331.62it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 7803.21it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 7140.09it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 7141.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the start and end of the desired time window in seconds\n",
    "start_time_window = 60*0 # 5 minutes\n",
    "end_time_window = 60*5 # 10 minutes\n",
    "\n",
    "all_sensory_spikes = []\n",
    "all_motor_spikes = []\n",
    "\n",
    "for i_chipid, chip_id in enumerate(chip_ids):\n",
    "    for i_chip_session, chip_session in enumerate(chip_sessions):\n",
    "        print('Loading for chip {}, session {}'.format(chip_id, chip_session))\n",
    "        try:\n",
    "            data_subset, events = utils_data.load_file(chip_id, chip_session, data_path)\n",
    "        except:\n",
    "            print(f'>>Could not load chip {chip_id}, session {chip_session}<<')\n",
    "            print('------------------------\\n')\n",
    "            continue\n",
    "        spiketimes = utils_data.get_spiketimes(data_subset, array_size,fs)\n",
    "        sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes = utils_data.get_electrode_regions(data_subset, spiketimes, do_plot = False)\n",
    "\n",
    "        all_spikes = [sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes]\n",
    "        max_time_ms = max(max(max(spikes) for spikes in spike_list)*1000 for spike_list in all_spikes)\n",
    "\n",
    "        sensory_spikes_binned = utils_tensor.spike_times_to_bins(sensory_spikes, binsize, max_time_ms, spike_tag = 'sensory')\n",
    "        up1_spikes_binned = utils_tensor.spike_times_to_bins(up1_spikes, binsize, max_time_ms, spike_tag = 'up1')\n",
    "        down1_spikes_binned = utils_tensor.spike_times_to_bins(down1_spikes, binsize, max_time_ms, spike_tag='down1')\n",
    "        up2_spikes_binned = utils_tensor.spike_times_to_bins(up2_spikes, binsize, max_time_ms, spike_tag = 'up2')\n",
    "        down2_spikes_binned = utils_tensor.spike_times_to_bins(down2_spikes, binsize, max_time_ms, spike_tag = 'down2')\n",
    "\n",
    "        # Determine how many bins correspond to the desired time window\n",
    "        start_window_bins = int(start_time_window / (binsize/1000))\n",
    "        end_window_bins = int(end_time_window / (binsize/1000))\n",
    "        \n",
    "        # Slice the tensors\n",
    "        sensory_spikes_binned = sensory_spikes_binned[:,start_window_bins:end_window_bins]\n",
    "        motor_spikes_binned = torch.cat([up1_spikes_binned[:,start_window_bins:end_window_bins], \n",
    "                                        down1_spikes_binned[:,start_window_bins:end_window_bins], \n",
    "                                        up2_spikes_binned[:,start_window_bins:end_window_bins], \n",
    "                                        down2_spikes_binned[:,start_window_bins:end_window_bins]], dim = 0)\n",
    "\n",
    "        # Add the binned spikes to their respective lists\n",
    "        all_sensory_spikes.append(sensory_spikes_binned)\n",
    "        all_motor_spikes.append(motor_spikes_binned)\n",
    "        \n",
    "        print('------------------------\\n')\n",
    "\n",
    "# Concatenate all sensory and motor binned spikes into two separate tensors\n",
    "sensory_spikes_binned = torch.cat(all_sensory_spikes, dim=1)\n",
    "motor_spikes_binned = torch.cat(all_motor_spikes, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66c75eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 15])\n",
      "torch.Size([400, 15])\n"
     ]
    }
   ],
   "source": [
    "print(sensory_spikes_binned.shape)\n",
    "print(motor_spikes_binned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f4f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeDataset(Dataset):\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "        '''min_val = torch.min(tensor)\n",
    "        max_val = torch.max(tensor)\n",
    "        tensor = 2 * (tensor - min_val) / (max_val - min_val) - 1'''\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.tensor.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.tensor[:, index]\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Initialize the dataset for sensory spikes\n",
    "dataset_sensory = NormalizeDataset(sensory_spikes_binned)\n",
    "\n",
    "train_size_sensory = int(0.8 * len(dataset_sensory))\n",
    "test_size_sensory = len(dataset_sensory) - train_size_sensory\n",
    "train_dataset_sensory, test_dataset_sensory = random_split(dataset_sensory, [train_size_sensory, test_size_sensory])\n",
    "\n",
    "train_dataloader_sensory = DataLoader(train_dataset_sensory, batch_size=batch_size, shuffle=True,\n",
    "                                    num_workers = num_workers, pin_memory = pin_memory)\n",
    "test_dataloader_sensory = DataLoader(test_dataset_sensory, batch_size=batch_size, shuffle=False,\n",
    "                                    num_workers = num_workers, pin_memory = pin_memory)\n",
    "\n",
    "# Initialize the dataset for motor spikes\n",
    "dataset_motor = NormalizeDataset(motor_spikes_binned)\n",
    "\n",
    "train_size_motor = int(0.8 * len(dataset_motor))\n",
    "test_size_motor = len(dataset_motor) - train_size_motor\n",
    "train_dataset_motor, test_dataset_motor = random_split(dataset_motor, [train_size_motor, test_size_motor])\n",
    "\n",
    "train_dataloader_motor = DataLoader(train_dataset_motor, batch_size=batch_size, shuffle=True,\n",
    "                                    num_workers = num_workers, pin_memory = pin_memory)\n",
    "test_dataloader_motor = DataLoader(test_dataset_motor, batch_size=batch_size, shuffle=False,\n",
    "                                    num_workers = num_workers, pin_memory = pin_memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be136455",
   "metadata": {},
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb55624c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0  :  500  ->  256\n",
      "Layer 1  :  256  ->  128\n",
      "Layer 2  :  128  ->  256\n",
      "Layer 3  :  256  ->  400\n"
     ]
    }
   ],
   "source": [
    "# Building the network\n",
    "shapes = [sensory_spikes_binned.shape[0], L1_size, L2_size, L3_size, motor_spikes_binned.shape[0]]\n",
    "layers = []\n",
    "for i in range(len(shapes)-1) :\n",
    "    print('Layer', i, ' : ', shapes[i], ' -> ', shapes[i+1])\n",
    "    layers.append(pcn.FCLayer(input_size = shapes[i],\n",
    "                            output_size = shapes[i+1], f = f, df = df,\n",
    "                            device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8cd528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pcn.PCNet_Bogacz(layers = layers, batch_size = batch_size,\n",
    "                        n_inferences_steps = n_inferences_steps,\n",
    "                        mu_lr = mu_lr, theta_lr = theta_lr, pi_lr = 42,\n",
    "                        fixed_predictions = fixed_predictions, update_weights_flag=update_weights_flag,\n",
    "                        weight_clamp = weight_clamp, mu_clamp = mu_clamp,  pi_clamp = 42,\n",
    "                        device = device, do_pi = False,\n",
    "                        mode = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "530cbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/5 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 11144, 18196) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1164\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\2023-07-10b_pcn_spikes_prepost.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10b_pcn_spikes_prepost.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# GPU burning\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10b_pcn_spikes_prepost.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m losses, accs, test_accs \u001b[39m=\u001b[39m pcn\u001b[39m.\u001b[39;49mtrain_mse(model \u001b[39m=\u001b[39;49m model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10b_pcn_spikes_prepost.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                         inputs_dataloader \u001b[39m=\u001b[39;49m train_dataloader_sensory, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10b_pcn_spikes_prepost.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                         outputs_dataloader \u001b[39m=\u001b[39;49m train_dataloader_motor,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10b_pcn_spikes_prepost.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                         test_inputs_dataloader \u001b[39m=\u001b[39;49m test_dataloader_sensory,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10b_pcn_spikes_prepost.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                         test_outputs_dataloader \u001b[39m=\u001b[39;49m test_dataloader_motor, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10b_pcn_spikes_prepost.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                         n_epochs \u001b[39m=\u001b[39;49m epochs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10b_pcn_spikes_prepost.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m utils_pcn\u001b[39m.\u001b[39mplot_loss_accs(losses, accs, test_accs)\n",
      "File \u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\../code/models\\pcn.py:234\u001b[0m, in \u001b[0;36mtrain_mse\u001b[1;34m(model, inputs_dataloader, outputs_dataloader, test_inputs_dataloader, test_outputs_dataloader, n_epochs)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_epochs), total\u001b[39m=\u001b[39mn_epochs, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining...\u001b[39m\u001b[39m'\u001b[39m, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    233\u001b[0m     losslist, acclist \u001b[39m=\u001b[39m [], []\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(inputs_dataloader, outputs_dataloader):\n\u001b[0;32m    235\u001b[0m         \u001b[39m# Transpose inputs and targets to match batch_size x n_neurons\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    237\u001b[0m         targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1324\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1325\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1326\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1327\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1176\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1175\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1176\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   1178\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 11144, 18196) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# GPU burning\n",
    "losses, accs, test_accs = pcn.train_mse(model = model,\n",
    "                                        inputs_dataloader = train_dataloader_sensory, \n",
    "                                        outputs_dataloader = train_dataloader_motor,\n",
    "                                        test_inputs_dataloader = test_dataloader_sensory,\n",
    "                                        test_outputs_dataloader = test_dataloader_motor, \n",
    "                                        n_epochs = epochs)\n",
    "\n",
    "utils_pcn.plot_loss_accs(losses, accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9be3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And generative mode \n",
    "one_hot_matrix = torch.eye(10, device = device)\n",
    "labels = torch.randint(10, size = (batch_size,), device = device)\n",
    "one_hot_vectors = one_hot_matrix[labels]\n",
    "gen_imgs = model.hard_generate(y = one_hot_vectors, \n",
    "                            n_generative_steps = n_generative_steps) \n",
    "\n",
    "fig, axs = plt.subplots(figsize = (10,10), ncols = np.sqrt(batch_size).astype(int), nrows = np.sqrt(batch_size).astype(int))\n",
    "for iax, ax in enumerate(axs.flatten()) :\n",
    "    ax.imshow(gen_imgs[iax,:].reshape(28,28).cpu().detach().numpy(), cmap = 'gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc48b7f97eeefcfa973ac84946cdeb32dcd8538d584fc23cdbd11e050afa8c03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
