{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import psweep as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/utils')  # Add the utils directory to the Python path\n",
    "import utils_data, utils_spikes, utils_events, utils_tensor, utils_pcn \n",
    "\n",
    "sys.path.append('../code/models')  # Add the models directory to the Python path\n",
    "import pcn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91b1c31",
   "metadata": {},
   "source": [
    "# Well I have to admit I didn't think it would work this good\n",
    "## Now for the final piece : getting spikes from commands \n",
    "### We will train the PCN on \"expert dishes\" (15-20mn), then get the motor spikes that are either \"up\" or \"down\" commands, and use the reverse backwards mode to generate spikes for optimal sensory control\n",
    "### We will also do it on \"naive dishes\" (0-5mn) to compare how orthogonal the up-down representations are "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac8d02",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406af944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment specific parameters \n",
    "chip_ids = [9501, 11614, 11615] # experiment ID\n",
    "chip_sessions = [0,2] # 2 for post-training, 0 for pre-training\n",
    "\n",
    "# Stable parameters\n",
    "data_path = '../data/cortical_labs_data/' # path to data\n",
    "fs = 20000 # sampling frequency\n",
    "binsize = 100 # ms, bin size for spike counts\n",
    "array_size = 1024 # number of electrode in the array\n",
    "\n",
    "# Torch parameters \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "num_workers = 2\n",
    "pin_memory = False\n",
    "\n",
    "# Learn parameters\n",
    "batch_size = 32\n",
    "epochs = 5  # should be enough given the 07-07 notebook\n",
    "n_inferences_steps = 500  # number of inference steps per input\n",
    "n_generative_steps = 500 # number of generative steps per input\n",
    "\n",
    "# Layer parameters\n",
    "update_weights_flag = True  # whether to learn the FF weights\n",
    "f = utils_pcn.tanh\n",
    "df = utils_pcn.tanh_deriv\n",
    "\n",
    "# Network parameters\n",
    "fixed_predictions = True  # change the predictions or not\n",
    "theta_lr = 5e-4\n",
    "mu_lr = 5e-2\n",
    "weight_clamp = 50  # weight clamp\n",
    "mu_clamp = 1000  # value neuron clamp\n",
    "\n",
    "L1_size = 256\n",
    "L2_size = 128\n",
    "L3_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab49835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE MODEL WITH SPIKES ON THE TIME WINDOW :  0  -  300  s\n",
      "Loading for chip 9501, session 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 27.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 5547.93it/s]\n",
      "c:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\../code/utils\\utils_tensor.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  return torch.tensor(binned_spikes)\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 5861.57it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 4882.26it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 4939.12it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 5750.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      "Loading for chip 9501, session 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 5717.61it/s]\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 5576.49it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 5880.80it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 5554.34it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 5788.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      "Loading for chip 11614, session 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 26.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 5837.23it/s]\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 5492.66it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 5865.09it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 5549.27it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 5708.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      "Loading for chip 11614, session 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 25.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 5512.39it/s]\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 5890.38it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 5920.06it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 6660.06it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 6248.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      "Loading for chip 11615, session 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 25.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Could not load chip 11615, session 0<<\n",
      "------------------------\n",
      "\n",
      "Loading for chip 11615, session 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 26.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 6420.77it/s]\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 8518.77it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 7810.62it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 6664.93it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 6362.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      "Layer 0  :  500  ->  256\n",
      "Layer 1  :  256  ->  128\n",
      "Layer 2  :  128  ->  256\n",
      "Layer 3  :  256  ->  400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/5 [01:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\2023-07-11_final_masterpiece.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m model \u001b[39m=\u001b[39m pcn\u001b[39m.\u001b[39mPCNet_Bogacz(layers \u001b[39m=\u001b[39m layers, batch_size \u001b[39m=\u001b[39m batch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m                 n_inferences_steps \u001b[39m=\u001b[39m n_inferences_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m                 mu_lr \u001b[39m=\u001b[39m mu_lr, theta_lr \u001b[39m=\u001b[39m theta_lr, pi_lr \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m                 device \u001b[39m=\u001b[39m device, do_pi \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m                 mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# GPU burning\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m losses, accs, test_accs \u001b[39m=\u001b[39m pcn\u001b[39m.\u001b[39;49mtrain_mse(model \u001b[39m=\u001b[39;49m model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m                                         inputs_dataloader \u001b[39m=\u001b[39;49m train_dataloader_sensory, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m                                         outputs_dataloader \u001b[39m=\u001b[39;49m train_dataloader_motor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m                                         test_inputs_dataloader \u001b[39m=\u001b[39;49m test_dataloader_sensory,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m                                         test_outputs_dataloader \u001b[39m=\u001b[39;49m test_dataloader_motor, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m                                         n_epochs \u001b[39m=\u001b[39;49m epochs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m utils_pcn\u001b[39m.\u001b[39mplot_loss_accs(losses, accs, test_accs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-11_final_masterpiece.ipynb#W5sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m# -- Run in generative model -- # \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\../code/models\\pcn.py:240\u001b[0m, in \u001b[0;36mtrain_mse\u001b[1;34m(model, inputs_dataloader, outputs_dataloader, test_inputs_dataloader, test_outputs_dataloader, n_epochs)\u001b[0m\n\u001b[0;32m    237\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    238\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 240\u001b[0m squared_L, acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_pc(inputs, targets)\n\u001b[0;32m    242\u001b[0m losslist\u001b[39m.\u001b[39mappend(squared_L)\n\u001b[0;32m    243\u001b[0m acclist\u001b[39m.\u001b[39mappend(acc)\n",
      "File \u001b[1;32mc:\\Users\\skorm\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\../code/models\\pcn.py:124\u001b[0m, in \u001b[0;36mPCNet_Bogacz.run_pc\u001b[1;34m(self, inp, label)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_errors[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmus[i] \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mouts[i]\n\u001b[0;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_errors[i] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minhs[i]\n\u001b[1;32m--> 124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minhs[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minh_ws[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_errors[i]\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minhs[i]\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfixed_predictions:\n\u001b[0;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mouts[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mforward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmus[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time_windows = [60*0, 60*15]\n",
    "end_time_windows = [60*5, 60*20]\n",
    "\n",
    "all_up_batches, all_down_batches, all_models = [], [], []\n",
    "all_losses, all_accs, all_train_accs = [], [], []\n",
    "for itime_window in range(len(start_time_windows)):\n",
    "    print('RUNNING THE MODEL WITH SPIKES ON THE TIME WINDOW : ', start_time_windows[itime_window], ' - ', end_time_windows[itime_window], ' s')\n",
    "    \n",
    "    \n",
    "    # -- Getting the data -- #\n",
    "    start_time_window = start_time_windows[itime_window]\n",
    "    end_time_window = end_time_windows[itime_window]\n",
    "    # PEP-8 is crying seeing the line below\n",
    "    sensory_spikes_binned, motor_spikes_binned, up_spikes_binned, down_spikes_binned = utils_data.get_all_spikes_time_window(start_time_window, end_time_window,\n",
    "                                                                                                                        binsize, chip_ids, chip_sessions,\n",
    "                                                                                                                        data_path, array_size, fs)\n",
    "    \n",
    "    # -- Getting the up-down labels -- #\n",
    "    total_up_spikes = up_spikes_binned.sum(dim=0)\n",
    "    total_down_spikes = down_spikes_binned.sum(dim=0)\n",
    "\n",
    "    up_timesteps = total_up_spikes > total_down_spikes\n",
    "    down_timesteps = total_down_spikes > total_up_spikes\n",
    "\n",
    "    labels = torch.empty(motor_spikes_binned.shape[1], dtype=torch.long)\n",
    "    labels[up_timesteps] = 1\n",
    "    labels[down_timesteps] = 0\n",
    "    \n",
    "    \n",
    "    # -- Make the dataset -- #\n",
    "    # Initialize the dataset for sensory spikes\n",
    "    dataset_sensory = utils_tensor.NormalizeDataset(sensory_spikes_binned)\n",
    "\n",
    "    train_size_sensory = int(0.8 * len(dataset_sensory))\n",
    "    test_size_sensory = len(dataset_sensory) - train_size_sensory\n",
    "    train_dataset_sensory, test_dataset_sensory = random_split(dataset_sensory, [train_size_sensory, test_size_sensory])\n",
    "\n",
    "    train_dataloader_sensory = DataLoader(train_dataset_sensory, batch_size=batch_size, shuffle=True,\n",
    "                                        num_workers = num_workers, pin_memory = pin_memory)\n",
    "    test_dataloader_sensory = DataLoader(test_dataset_sensory, batch_size=batch_size, shuffle=False,\n",
    "                                        num_workers = num_workers, pin_memory = pin_memory)\n",
    "\n",
    "    # Initialize the dataset for motor spikes\n",
    "    dataset_motor = utils_tensor.NormalizeDataset(motor_spikes_binned)\n",
    "\n",
    "    train_size_motor = int(0.8 * len(dataset_motor))\n",
    "    test_size_motor = len(dataset_motor) - train_size_motor\n",
    "    train_dataset_motor, test_dataset_motor = random_split(dataset_motor, [train_size_motor, test_size_motor])\n",
    "\n",
    "    train_dataloader_motor = DataLoader(train_dataset_motor, batch_size=batch_size, shuffle=True,\n",
    "                                        num_workers = num_workers, pin_memory = pin_memory)\n",
    "    test_dataloader_motor = DataLoader(test_dataset_motor, batch_size=batch_size, shuffle=False,\n",
    "                                        num_workers = num_workers, pin_memory = pin_memory)\n",
    "    \n",
    "    \n",
    "    # -- Make the model -- #\n",
    "    shapes = [sensory_spikes_binned.shape[0], L1_size, L2_size, L3_size, motor_spikes_binned.shape[0]]\n",
    "    layers = []\n",
    "    for i in range(len(shapes)-1) :\n",
    "        print('Layer', i, ' : ', shapes[i], ' -> ', shapes[i+1])\n",
    "        layers.append(pcn.FCLayer(input_size = shapes[i],\n",
    "                                output_size = shapes[i+1], f = f, df = df,\n",
    "                                device = device))\n",
    "        \n",
    "    model = pcn.PCNet_Bogacz(layers = layers, batch_size = batch_size,\n",
    "                    n_inferences_steps = n_inferences_steps,\n",
    "                    mu_lr = mu_lr, theta_lr = theta_lr, pi_lr = 42,\n",
    "                    fixed_predictions = fixed_predictions, update_weights_flag=update_weights_flag,\n",
    "                    weight_clamp = weight_clamp, mu_clamp = mu_clamp,  pi_clamp = 42,\n",
    "                    device = device, do_pi = False,\n",
    "                    mode = 'mse')\n",
    "    \n",
    "    # GPU burning\n",
    "    losses, accs, test_accs = pcn.train_mse(model = model,\n",
    "                                            inputs_dataloader = train_dataloader_sensory, \n",
    "                                            outputs_dataloader = train_dataloader_motor,\n",
    "                                            test_inputs_dataloader = test_dataloader_sensory,\n",
    "                                            test_outputs_dataloader = test_dataloader_motor, \n",
    "                                            n_epochs = epochs)\n",
    "\n",
    "    utils_pcn.plot_loss_accs(losses, accs, test_accs)\n",
    "    \n",
    "    \n",
    "    # -- Run in generative model -- # \n",
    "    n_batches = 50 # we select 50 batches of either up or down spikes pattern for generation \n",
    "    up_indices = torch.where(labels == 1)[0]\n",
    "    down_indices = torch.where(labels == 0)[0]\n",
    "\n",
    "    up_batches, down_batches  = [], []\n",
    "    for _ in tqdm(range(n_batches), desc='Generating up batches of spikes'):\n",
    "        # Randomly permute the indices.\n",
    "        permuted_up_indices = up_indices[torch.randperm(up_indices.size()[0])]\n",
    "        selected_indices = permuted_up_indices[:batch_size]\n",
    "\n",
    "        # Create a mask for the selected indices\n",
    "        mask = torch.ones(up_indices.size()[0], dtype=bool)\n",
    "        for index in selected_indices:\n",
    "            mask[torch.where(up_indices == index)] = False\n",
    "        up_indices = up_indices[mask]\n",
    "\n",
    "        # Select the corresponding \"up\" spike patterns.\n",
    "        selected_up_spikes = motor_spikes_binned[:, selected_indices]\n",
    "        gen_firing_rates = model.hard_generate(y=selected_up_spikes, \n",
    "                                                n_generative_steps=n_generative_steps)\n",
    "        up_batches.append(gen_firing_rates)\n",
    "        \n",
    "    # Now the same for the \"down\" spike patterns.\n",
    "    for _ in tqdm(range(n_batches), desc='Generating down batches of spikes'):\n",
    "        # Randomly permute the indices.\n",
    "        permuted_down_indices = down_indices[torch.randperm(down_indices.size()[0])]\n",
    "        selected_indices = permuted_down_indices[:batch_size]\n",
    "\n",
    "        # Create a mask for the selected indices\n",
    "        mask = torch.ones(down_indices.size()[0], dtype=bool)\n",
    "        for index in selected_indices:\n",
    "            mask[torch.where(down_indices == index)] = False\n",
    "        down_indices = down_indices[mask]\n",
    "\n",
    "        # Select the corresponding \"up\" spike patterns.\n",
    "        selected_down_spikes = motor_spikes_binned[:, selected_indices]\n",
    "        gen_firing_rates = model.hard_generate(y=selected_down_spikes, \n",
    "                                                n_generative_steps=n_generative_steps)\n",
    "        down_batches.append(gen_firing_rates)\n",
    "        \n",
    "    up_batches = torch.stack(up_batches).cpu().numpy()\n",
    "    down_batches = torch.stack(down_batches).cpu().numpy()\n",
    "    \n",
    "    similarities = []\n",
    "    for vec1, vec2 in zip(up_batches, down_batches):\n",
    "        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    plt.hist(similarities, bins=10)\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Pairwise Similarities')\n",
    "    plt.show()\n",
    "    \n",
    "    all_up_batches.append(up_batches)\n",
    "    all_down_batches.append(down_batches)\n",
    "    all_models.append(model)\n",
    "    all_losses.append(losses)\n",
    "    all_train_accs.append(accs)\n",
    "    all_accs.append(test_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc48b7f97eeefcfa973ac84946cdeb32dcd8538d584fc23cdbd11e050afa8c03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
