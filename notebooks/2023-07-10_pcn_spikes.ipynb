{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import psweep as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/utils')  # Add the utils directory to the Python path\n",
    "import utils_data, utils_spikes, utils_events, utils_tensor, utils_pcn \n",
    "\n",
    "sys.path.append('../code/models')  # Add the models directory to the Python path\n",
    "import pcn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91b1c31",
   "metadata": {},
   "source": [
    "# Seems to be working-ish, now we can use it to make spikes\n",
    "# We'll get sensory neurons as inputs, and motor neurons as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406af944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment specific parameters \n",
    "chip_id = 9501 # experiment ID\n",
    "chip_session = 0 # 2 for post-training, 0 for pre-training\n",
    "\n",
    "# Stable parameters\n",
    "data_path = '../data/cortical_labs_data/' # path to data\n",
    "fs = 20000 # sampling frequency\n",
    "binsize = 100 # ms, bin size for spike counts\n",
    "array_size = 1024 # number of electrode in the array\n",
    "\n",
    "# Torch parameters \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "num_workers = 18\n",
    "pin_memory = True\n",
    "\n",
    "# Learn parameters\n",
    "batch_size = 32\n",
    "epochs = 5  # should be enough given the 07-07 notebook\n",
    "n_inferences_steps = 500  # number of inference steps per input\n",
    "n_generative_steps = 500 # number of generative steps per input\n",
    "\n",
    "# Layer parameters\n",
    "update_weights_flag = True  # whether to learn the FF weights\n",
    "f = utils_pcn.tanh\n",
    "df = utils_pcn.tanh_deriv\n",
    "\n",
    "# Network parameters\n",
    "fixed_predictions = True  # change the predictions or not\n",
    "theta_lr = 5e-4\n",
    "mu_lr = 5e-2\n",
    "weight_clamp = 50  # weight clamp\n",
    "mu_clamp = 1000  # value neuron clamp\n",
    "\n",
    "L1_size = 256\n",
    "L2_size = 128\n",
    "L3_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab49835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|██████████| 29/29 [00:01<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulation mode: full game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning sensory channels: 100%|██████████| 500/500 [00:00<00:00, 5374.64it/s]\n",
      "c:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\../code/utils\\utils_tensor.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  return torch.tensor(binned_spikes)\n",
      "Binning up1 channels: 100%|██████████| 100/100 [00:00<00:00, 6169.64it/s]\n",
      "Binning down1 channels: 100%|██████████| 100/100 [00:00<00:00, 5882.61it/s]\n",
      "Binning up2 channels: 100%|██████████| 100/100 [00:00<00:00, 5554.34it/s]\n",
      "Binning down2 channels: 100%|██████████| 100/100 [00:00<00:00, 5273.47it/s]\n"
     ]
    }
   ],
   "source": [
    "data_subset, events = utils_data.load_file(chip_id, chip_session, data_path)\n",
    "spiketimes = utils_data.get_spiketimes(data_subset, array_size,fs)\n",
    "sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes = utils_data.get_electrode_regions(data_subset, spiketimes, do_plot = False)\n",
    "\n",
    "all_spikes = [sensory_spikes, up1_spikes, up2_spikes, down1_spikes, down2_spikes]\n",
    "# Find maximum time across all spike lists\n",
    "max_time_ms = max(max(max(spikes) for spikes in spike_list)*1000 for spike_list in all_spikes)\n",
    "\n",
    "# Create binned spikes tensor for each region\n",
    "sensory_spikes_binned = utils_tensor.spike_times_to_bins(sensory_spikes, binsize, max_time_ms, spike_tag = 'sensory')\n",
    "up1_spikes_binned = utils_tensor.spike_times_to_bins(up1_spikes, binsize, max_time_ms, spike_tag = 'up1')\n",
    "down1_spikes_binned = utils_tensor.spike_times_to_bins(down1_spikes, binsize, max_time_ms, spike_tag='down1')\n",
    "up2_spikes_binned = utils_tensor.spike_times_to_bins(up2_spikes, binsize, max_time_ms, spike_tag = 'up2')\n",
    "down2_spikes_binned = utils_tensor.spike_times_to_bins(down2_spikes, binsize, max_time_ms, spike_tag = 'down2')\n",
    "motor_spikes_binned = torch.concat([up1_spikes_binned, down1_spikes_binned, up2_spikes_binned, down2_spikes_binned], dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc3a2037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 12001])\n",
      "torch.Size([400, 12001])\n"
     ]
    }
   ],
   "source": [
    "print(sensory_spikes_binned.shape)\n",
    "print(motor_spikes_binned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f4f53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skorm\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:563: UserWarning: This DataLoader will create 18 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "class NormalizeDataset(Dataset):\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "        '''min_val = torch.min(tensor)\n",
    "        max_val = torch.max(tensor)\n",
    "        tensor = 2 * (tensor - min_val) / (max_val - min_val) - 1'''\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.tensor.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.tensor[:, index]\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Initialize the dataset for sensory spikes\n",
    "dataset_sensory = NormalizeDataset(sensory_spikes_binned)\n",
    "\n",
    "train_size_sensory = int(0.8 * len(dataset_sensory))\n",
    "test_size_sensory = len(dataset_sensory) - train_size_sensory\n",
    "train_dataset_sensory, test_dataset_sensory = random_split(dataset_sensory, [train_size_sensory, test_size_sensory])\n",
    "\n",
    "train_dataloader_sensory = DataLoader(train_dataset_sensory, batch_size=batch_size, shuffle=True,\n",
    "                                    num_workers = num_workers, pin_memory = pin_memory)\n",
    "test_dataloader_sensory = DataLoader(test_dataset_sensory, batch_size=batch_size, shuffle=False,\n",
    "                                    num_workers = num_workers, pin_memory = pin_memory)\n",
    "\n",
    "# Initialize the dataset for motor spikes\n",
    "dataset_motor = NormalizeDataset(motor_spikes_binned)\n",
    "\n",
    "train_size_motor = int(0.8 * len(dataset_motor))\n",
    "test_size_motor = len(dataset_motor) - train_size_motor\n",
    "train_dataset_motor, test_dataset_motor = random_split(dataset_motor, [train_size_motor, test_size_motor])\n",
    "\n",
    "train_dataloader_motor = DataLoader(train_dataset_motor, batch_size=batch_size, shuffle=True,\n",
    "                                    num_workers = num_workers, pin_memory = pin_memory)\n",
    "test_dataloader_motor = DataLoader(test_dataset_motor, batch_size=batch_size, shuffle=False,\n",
    "                                    num_workers = num_workers, pin_memory = pin_memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be136455",
   "metadata": {},
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb55624c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0  :  500  ->  256\n",
      "Layer 1  :  256  ->  128\n",
      "Layer 2  :  128  ->  256\n",
      "Layer 3  :  256  ->  400\n"
     ]
    }
   ],
   "source": [
    "# Building the network\n",
    "shapes = [sensory_spikes_binned.shape[0], L1_size, L2_size, L3_size, motor_spikes_binned.shape[0]]\n",
    "layers = []\n",
    "for i in range(len(shapes)-1) :\n",
    "    print('Layer', i, ' : ', shapes[i], ' -> ', shapes[i+1])\n",
    "    layers.append(pcn.FCLayer(input_size = shapes[i],\n",
    "                            output_size = shapes[i+1], f = f, df = df,\n",
    "                            device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8cd528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pcn.PCNet_Bogacz(layers = layers, batch_size = batch_size,\n",
    "                        n_inferences_steps = n_inferences_steps,\n",
    "                        mu_lr = mu_lr, theta_lr = theta_lr, pi_lr = 42,\n",
    "                        fixed_predictions = fixed_predictions, update_weights_flag=update_weights_flag,\n",
    "                        weight_clamp = weight_clamp, mu_clamp = mu_clamp,  pi_clamp = 42,\n",
    "                        device = device, do_pi = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "530cbb69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\skorm\\Documents\\GitHub\\BrainDishSiMulator\\notebooks\\2023-07-10_pcn_spikes.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10_pcn_spikes.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# GPU burning\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10_pcn_spikes.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m losses, accs, test_accs \u001b[39m=\u001b[39m pcn\u001b[39m.\u001b[39mtrain(model \u001b[39m=\u001b[39m model, \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10_pcn_spikes.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                 trainset \u001b[39m=\u001b[39m trainset[\u001b[39m0\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], testset \u001b[39m=\u001b[39m testset[\u001b[39m0\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10_pcn_spikes.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                 n_epochs \u001b[39m=\u001b[39m epochs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10_pcn_spikes.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m utils_pcn\u001b[39m.\u001b[39mplot_loss_accs(losses, accs, test_accs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skorm/Documents/GitHub/BrainDishSiMulator/notebooks/2023-07-10_pcn_spikes.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# And generative mode \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainset' is not defined"
     ]
    }
   ],
   "source": [
    "# GPU burning\n",
    "losses, accs, test_accs = pcn.train(model = model, \n",
    "                                trainset = trainset[0:-2], testset = testset[0:-2],\n",
    "                                n_epochs = epochs)\n",
    "\n",
    "utils_pcn.plot_loss_accs(losses, accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9be3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And generative mode \n",
    "one_hot_matrix = torch.eye(10, device = device)\n",
    "labels = torch.randint(10, size = (batch_size,), device = device)\n",
    "one_hot_vectors = one_hot_matrix[labels]\n",
    "gen_imgs = model.hard_generate(y = one_hot_vectors, \n",
    "                            n_generative_steps = n_generative_steps) \n",
    "\n",
    "fig, axs = plt.subplots(figsize = (10,10), ncols = np.sqrt(batch_size).astype(int), nrows = np.sqrt(batch_size).astype(int))\n",
    "for iax, ax in enumerate(axs.flatten()) :\n",
    "    ax.imshow(gen_imgs[iax,:].reshape(28,28).cpu().detach().numpy(), cmap = 'gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc48b7f97eeefcfa973ac84946cdeb32dcd8538d584fc23cdbd11e050afa8c03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
